# Copyright (c) Facebook, Inc. and its affiliates. All Rights Reserved
#
#### models ####
arch: 'attentive_nas_static_model'

pareto_models:
    supernet_checkpoint_path: "./attentive_nas_data/attentive_nas_pretrained.pth.tar"

    a0:
        model: "attentive_nas_a0"
        resolution: 192
        width: [16, 16, 24, 32, 64, 112, 192, 216, 1792]
        kernel_size: [3, 3, 3, 3, 3, 3, 3]
        expand_ratio: [1, 4, 4, 4, 4, 6, 6]
        depth: [1, 3, 3, 3, 3, 3, 1]

    a1: 
        model: "attentive_nas_a1"
        resolution: 224
        width: [16, 16, 24, 32, 64, 112, 192, 216, 1984]
        kernel_size: [3, 3, 3, 5, 3, 5, 3]
        expand_ratio: [1, 4, 4, 4, 4, 6, 6]
        depth: [1, 3, 3, 3, 3, 3, 1]

    a2:
        model: "attentive_nas_a2"
        resolution: 224
        width: [16, 16, 24, 32, 64, 112, 200, 224, 1984]
        kernel_size: [3, 3, 3, 3, 3, 5, 3]
        expand_ratio: [1, 4, 5, 4, 4, 6, 6]
        depth: [1, 3, 3, 3, 3, 4, 1]

    a3:
        model: "attentive_nas_a3"
        resolution: 224
        width: [16, 16, 24, 32, 64, 112, 208, 224, 1984]
        kernel_size: [3, 3, 3, 5, 3, 3, 3]
        expand_ratio: [1, 4, 4, 4, 4, 6, 6]
        depth: [2, 3, 3, 4, 3, 5, 1]

    a4:
        model: "attentive_nas_a4"
        resolution: 256
        width: [16, 16, 24, 32, 64, 112, 192, 216, 1984]
        kernel_size: [3, 3, 3, 5, 3, 5, 3]
        expand_ratio: [1, 4, 4, 5, 4, 6, 6]
        depth: [1, 3, 3, 4, 3, 5, 1]

    a5:
        model: "attentive_nas_a5"
        resolution: 256
        width: [16, 16, 24, 32, 72, 112, 192, 216, 1792]
        kernel_size: [3, 3, 3, 5, 3, 3, 3]
        expand_ratio: [1, 4, 5, 4, 4, 6, 6]
        depth: [1, 3, 3, 3, 4, 6, 1]

    a6:
        model: "attentive_nas_a6"
        resolution: 288
        width: [16, 16, 24, 32, 64, 112, 216, 224, 1984]
        kernel_size: [3, 3, 3, 3, 3, 5, 3]
        expand_ratio: [1, 4, 6, 5, 4, 6, 6]
        depth: [1, 3, 3, 4, 4, 6, 1]


batch_size: 256
post_bn_calibration_batch_num: 64

augment: "auto_augment_tf"
interpolation: "bilinear"

bn_momentum: 0.1
bn_eps: 1e-5

distributed: False
distributed_val: False
eval_only: True

### imagenet dataset ###
dataset: 'imagenet'
dataset_dir: "/data/imagenet"
n_classes: 1000
drop_last: True
data_loader_workers_per_gpu: 4

print_freq: 10
seed: 2

#attentive nas search space
# c: channels, d: layers, k: kernel size, t: expand ratio, s: stride, act: activation, se: se layer
supernet_config:
    use_v3_head: True
    resolutions: [192, 224, 256, 288]
    first_conv: 
        c: [16, 24]
        act_func: 'swish'
        s: 2
    mb1:
        c: [16, 24]
        d: [1, 2]
        k: [3, 5]
        t: [1]
        s: 1
        act_func: 'swish'
        se: False
    mb2:
        c: [24, 32]
        d: [3, 4, 5]
        k: [3, 5]
        t: [4, 5, 6]
        s: 2
        act_func: 'swish'
        se: False
    mb3:
        c: [32, 40] 
        d: [3, 4, 5, 6]
        k: [3, 5]
        t: [4, 5, 6]
        s: 2
        act_func: 'swish'
        se: True
    mb4:
        c: [64, 72] 
        d: [3, 4, 5, 6]
        k: [3, 5]
        t: [4, 5, 6]
        s: 2
        act_func: 'swish'
        se: False
    mb5:
        c: [112, 120, 128] 
        d: [3, 4, 5, 6, 7, 8]
        k: [3, 5]
        t: [4, 5, 6]
        s: 1
        act_func: 'swish'
        se: True
    mb6:
        c: [192, 200, 208, 216] 
        d: [3, 4, 5, 6, 7, 8]
        k: [3, 5]
        t: [6]
        s: 2
        act_func: 'swish'
        se: True
    mb7:
        c: [216, 224] 
        d: [1, 2]
        k: [3, 5]
        t: [6]
        s: 1
        act_func: 'swish'
        se: True
    last_conv:
        c: [1792, 1984]
        act_func: 'swish'





